{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 从零实现BPE？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、BPE 分词器实现\n",
    "\n",
    "### 1.1 什么是BPE？\n",
    "\n",
    "BPE（Byte Pair Encoding，字节对编码）是一种常用的子词分词算法，广泛应用于自然语言处理任务中，尤其是在大规模预训练模型（如GPT、BERT等）中。\n",
    "BPE的核心思想是通过统计训练语料中出现频率最高的相邻字符对，并将其合并为新的符号，逐步构建出一个高效的子词词表。这样可以有效减少词表大小，同时兼顾词汇覆盖率和表示能力。\n",
    "\n",
    "BPE的主要流程如下：\n",
    "1. 将文本按字符切分，初始时每个字符为一个token。\n",
    "2. 统计所有相邻token对的出现频率，找到出现频率最高的token对。\n",
    "3. 将该token对合并为一个新的token，更新语料。\n",
    "4. 重复步骤2-3，直到达到预设的词表大小或没有高频token对可合并。\n",
    "\n",
    "BPE的优点在于能够处理未登录词（OOV），并且在词表规模和分词粒度之间取得平衡。它既能保留常见词的整体性，又能将罕见词拆分为更小的子词单元，从而提升模型的泛化能力。\n",
    "\n",
    "### 1.2 BPE算法示例：从字符到子词的合并过程  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始词表：\n",
      "l o w _: 3\n",
      "l o w e r _: 1\n",
      "l o w e s t _: 1\n",
      "\n",
      "第1轮合并: 合并('l', 'o'), 频率=5\n",
      "当前词表：\n",
      "lo w _: 3\n",
      "lo w e r _: 1\n",
      "lo w e s t _: 1\n",
      "\n",
      "第2轮合并: 合并('lo', 'w'), 频率=5\n",
      "当前词表：\n",
      "low _: 3\n",
      "low e r _: 1\n",
      "low e s t _: 1\n",
      "\n",
      "第3轮合并: 合并('low', '_'), 频率=3\n",
      "当前词表：\n",
      "low_: 3\n",
      "low e r _: 1\n",
      "low e s t _: 1\n",
      "\n",
      "最终BPE词表：\n",
      "{'low_', 'e', 't', 'r', 'low', '_', 's'}\n"
     ]
    }
   ],
   "source": [
    "# 实现BPE算法，输入case为low_ lower_ lowest_，用_标记结束\n",
    "\n",
    "from typing import Dict, Tuple, List, Set\n",
    "\n",
    "def get_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    统计词表中所有相邻符号对的频率\n",
    "\n",
    "    输入:\n",
    "        vocab: Dict[str, int]\n",
    "            词表，key为以空格分隔的token序列字符串，value为该词出现的频数\n",
    "\n",
    "            示例：\n",
    "            {\n",
    "                'l o w _': 1,\n",
    "                'l o w e r _': 1,\n",
    "                'l o w e s t _': 1\n",
    "            }\n",
    "    输出:\n",
    "        pairs: Dict[Tuple[str, str], int]\n",
    "            所有相邻token对及其在词表中的总出现频率\n",
    "\n",
    "            示例输出：\n",
    "            {\n",
    "                ('l', 'o'): 3,\n",
    "                ('o', 'w'): 3,\n",
    "                ('w', '_'): 1,\n",
    "                ('w', 'e'): 2,\n",
    "                ('e', 'r'): 1,\n",
    "                ('e', 's'): 1,\n",
    "                ('s', 't'): 1,\n",
    "                ('t', '_'): 1,\n",
    "                ('r', '_'): 1\n",
    "            }\n",
    "    \"\"\"\n",
    "    pairs: Dict[Tuple[str, str], int] = {}\n",
    "    for word, freq in vocab.items():\n",
    "        symbols: List[str] = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pairs[pair] = pairs.get(pair, 0) + freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair: Tuple[str, str], vocab: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    将词表中所有出现pair的地方合并为新符号\n",
    "\n",
    "    输入:\n",
    "        pair: Tuple[str, str]\n",
    "            需要合并的token对\n",
    "            示例：('l', 'o')\n",
    "        vocab: Dict[str, int]\n",
    "            当前词表，key为以空格分隔的token序列字符串，value为该词出现的频数\n",
    "\n",
    "            示例：\n",
    "            {\n",
    "                'l o w _': 1,\n",
    "                'l o w e r _': 1,\n",
    "                'l o w e s t _': 1\n",
    "            }\n",
    "\n",
    "    输出:\n",
    "        new_vocab: Dict[str, int]\n",
    "            合并pair后的新词表\n",
    "\n",
    "            示例输出（合并('l', 'o')后）：\n",
    "            {\n",
    "                'lo w _': 1,\n",
    "                'lo w e r _': 1,\n",
    "                'lo w e s t _': 1\n",
    "            }\n",
    "    \"\"\"\n",
    "    new_vocab: Dict[str, int] = {}\n",
    "    bigram: str = ' '.join(pair)\n",
    "    replacement: str = ''.join(pair)\n",
    "    for word in vocab:\n",
    "        # 用空格分隔，保证只合并相邻的pair\n",
    "        new_word: str = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "# 输入case\n",
    "corpus: List[str] = ['low_', 'lower_', 'lowest_']\n",
    "\n",
    "# 初始化词表（以字符为单位，空格分隔）\n",
    "vocab: Dict[str, int] = {}\n",
    "for word in corpus:\n",
    "    chars: str = ' '.join(list(word))\n",
    "    vocab[chars] = vocab.get(chars, 0) + 1\n",
    "\n",
    "print(\"初始词表：\")\n",
    "for k, v in vocab.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# vocab此时的示例内容：\n",
    "# l o w _: 1\n",
    "# l o w e r _: 1\n",
    "# l o w e s t _: 1\n",
    "\n",
    "num_merges: int = 3  # 最大合并次数，可根据需要调整\n",
    "for i in range(num_merges):\n",
    "    pairs: Dict[Tuple[str, str], int] = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    # 选择出现频率最高的pair\n",
    "    best: Tuple[str, str] = max(pairs, key=lambda x: pairs[x])\n",
    "    print(f\"\\n第{i+1}轮合并: 合并{best}, 频率={pairs[best]}\")\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(\"当前词表：\")\n",
    "    for k, v in vocab.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n最终BPE词表：\")\n",
    "tokens: Set[str] = set()\n",
    "for word in vocab:\n",
    "    tokens.update(word.split())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 BPE与字符压缩的关系\n",
    "- BPE（Byte Pair Encoding）本质上是一种数据压缩算法，其核心目标是通过合并高频子串（字符或子词）​来减少文本的存储空间或编码长度，从而提升后续处理（如NLP模型训练）的效率。​字符压缩率可以直观反映BPE的效果——即原始文本经过BPE编码后，平均每个字符或 token 所占用的空间是否减少。\n",
    "\n",
    "- 理论上，BPE（Byte Pair Encoding）分词的目标就是用词表（tokens）对单词进行最少分割，也就是让token数最少。这其实就是一个最优分割问题，经典做法是用动态规划（DP）/Bellman Principle（贝尔曼原理）来求解最小分割次数。下面代码是用“贪心法”——每次都尽量匹配最长的token。这种方法简单高效，但不一定总能得到最优（最少token数）分割。有些情况下，贪心法会错过全局最优解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "每个单词的字符压缩占比：\n",
      "low_: 原始字符数=4, BPE token数=1, 压缩占比=0.25\n",
      "low_: 原始字符数=4, BPE token数=1, 压缩占比=0.25\n",
      "low_: 原始字符数=4, BPE token数=1, 压缩占比=0.25\n",
      "lower_: 原始字符数=6, BPE token数=4, 压缩占比=0.67\n",
      "lowest_: 原始字符数=7, BPE token数=5, 压缩占比=0.71\n"
     ]
    }
   ],
   "source": [
    "# 计算每个单词BPE编码后的token数与原始字符数的压缩占比\n",
    "print(\"\\n每个单词的字符压缩占比：\")\n",
    "for word in corpus:\n",
    "    # 原始字符数\n",
    "    orig_len = len(word)\n",
    "    # BPE分词：从最终tokens集合中，尽可能长地匹配\n",
    "    chars = list(word)\n",
    "    bpe_tokens = []\n",
    "    i = 0\n",
    "    while i < len(chars):\n",
    "        matched = None\n",
    "        # 尝试最长匹配\n",
    "        for j in range(len(chars), i, -1):\n",
    "            sub = ' '.join(chars[i:j])\n",
    "            if sub.replace(' ', '') in tokens or sub in tokens:\n",
    "                matched = sub\n",
    "                bpe_tokens.append(sub)\n",
    "                i = j - 1\n",
    "                break\n",
    "        if matched is None:\n",
    "            # 单字符\n",
    "            bpe_tokens.append(chars[i])\n",
    "        i += 1\n",
    "    bpe_token_count = len(bpe_tokens)\n",
    "    compress_ratio = bpe_token_count / orig_len\n",
    "    print(f\"{word}: 原始字符数={orig_len}, BPE token数={bpe_token_count}, 压缩占比={compress_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、作业说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 代码结构\n",
    "\n",
    "1. 下载`https://github.com/stanford-cs336/assignment1-basics`代码\n",
    "2. 阅读`readme.md`安装数据集、环境 \n",
    "\n",
    "``` text \n",
    "assignment1-basics/\n",
    "├── README.md\n",
    "├── pyproject.toml / requirements.txt   # 依赖管理\n",
    "├── data/                               # 存放数据集\n",
    "│   ├── TinyStoriesV2-GPT4-train.txt\n",
    "│   ├── TinyStoriesV2-GPT4-valid.txt\n",
    "│   ├── owt_train.txt\n",
    "│   └── owt_valid.txt\n",
    "├── tests/\n",
    "│   ├── adapters.py                     # 适配器接口（你要实现的函数）\n",
    "│   ├── conftest.py                     # pytest 配置文件\n",
    "│   └── test_*.py                       # 具体测试文件（如 test_bpe.py、test_tokenizer.py 等）\n",
    "├── your_code.py / my_bpe.py / ...      # 你自己的实现文件（可选）\n",
    "└── ...                                 # 其他辅助文件\n",
    "```\n",
    "\n",
    "### 2.2 测试函数\n",
    "运行`uv run pytest`的输出，目前由于所有的核心函数都没有实现，抛出的错误都用`NotImplementedError`，作业1的BPE的部分，需要通过tests/test_tokenizer.py的所有测试函数。\n",
    "\n",
    "```text \n",
    "================================================ short test summary info =================================================\n",
    "FAILED tests/test_data.py::test_get_batch - NotImplementedError\n",
    "FAILED tests/test_model.py::test_linear - NotImplementedError\n",
    "FAILED tests/test_model.py::test_embedding - NotImplementedError\n",
    "FAILED tests/test_model.py::test_swiglu - NotImplementedError\n",
    "FAILED tests/test_model.py::test_scaled_dot_product_attention - NotImplementedError\n",
    "FAILED tests/test_model.py::test_4d_scaled_dot_product_attention - NotImplementedError\n",
    "FAILED tests/test_model.py::test_multihead_self_attention - NotImplementedError\n",
    "FAILED tests/test_model.py::test_multihead_self_attention_with_rope - NotImplementedError\n",
    "FAILED tests/test_model.py::test_transformer_lm - NotImplementedError\n",
    "FAILED tests/test_model.py::test_transformer_lm_truncated_input - NotImplementedError\n",
    "FAILED tests/test_model.py::test_transformer_block - NotImplementedError\n",
    "FAILED tests/test_model.py::test_rmsnorm - NotImplementedError\n",
    "FAILED tests/test_model.py::test_rope - NotImplementedError\n",
    "FAILED tests/test_model.py::test_silu_matches_pytorch - NotImplementedError\n",
    "FAILED tests/test_nn_utils.py::test_softmax_matches_pytorch - NotImplementedError\n",
    "FAILED tests/test_nn_utils.py::test_cross_entropy - NotImplementedError\n",
    "FAILED tests/test_nn_utils.py::test_gradient_clipping - NotImplementedError\n",
    "FAILED tests/test_optimizer.py::test_adamw - NotImplementedError\n",
    "FAILED tests/test_optimizer.py::test_get_lr_cosine_schedule - NotImplementedError\n",
    "FAILED tests/test_serialization.py::test_checkpointing - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_roundtrip_empty - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_empty_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_roundtrip_single_character - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_single_character_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_roundtrip_single_unicode_character - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_roundtrip_ascii_string - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_ascii_string_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_roundtrip_unicode_string - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_unicode_string_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_overlapping_special_tokens - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_address_roundtrip - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_address_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_german_roundtrip - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_german_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_tinystories_sample_roundtrip - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_tinystories_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_encode_special_token_trailing_newlines - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip - NotImplementedError\n",
    "FAILED tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken - NotImplementedError\n",
    "FAILED tests/test_train_bpe.py::test_train_bpe_speed - NotImplementedError\n",
    "FAILED tests/test_train_bpe.py::test_train_bpe - NotImplementedError\n",
    "FAILED tests/test_train_bpe.py::test_train_bpe_special_tokens - NotImplementedError\n",
    "============================================= 46 failed, 2 skipped in 2.51s ==============================================\n",
    "```\n",
    "\n",
    "### 环境配置\n",
    "`vscode`相关的编辑器需要在`.vscode`下新建`setting.json`配置pytest的相关配置，可以对每个测试函数单独进行调试，如下图所示\n",
    "```json\n",
    "{\n",
    "    \"python.testing.pytestArgs\": [\n",
    "        \"tests\"\n",
    "    ],\n",
    "    \"python.testing.unittestEnabled\": false,\n",
    "    \"python.testing.pytestEnabled\": true\n",
    "}\n",
    "```\n",
    "\n",
    "![VSCODE测试图](./image/1.1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 作业实现\n",
    "\n",
    "请实现如下图所示的一个Tokenizer类，注意合并阶段需要按照merge中给定的优先级合并，完成后即\n",
    "\n",
    "![Encode和Decode设计](./image/1.1_2.png)\n",
    "\n",
    "``` python \n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], special_tokens: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        初始化分词器。\n",
    "\n",
    "        参数:\n",
    "            vocab: 字典，键为token id（int），值为token对应的字节串（bytes）。\n",
    "            merges: BPE合并规则列表，每个元素为一个二元组（bytes, bytes）。\n",
    "            special_tokens: 可选，特殊token的字符串列表。\n",
    "\n",
    "        示例:\n",
    "            vocab = {0: b'a', 1: b'b', 2: b'ab'} \n",
    "            merges = [(b'a', b'b')]\n",
    "            special_tokens = ['<PAD>', '<EOS>']\n",
    "            tokenizer = Tokenizer(vocab, merges, special_tokens)\n",
    "        \"\"\"\n",
    "        pass  # 这里只是示例，实际实现略\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        将输入文本编码为token id的列表。\n",
    "\n",
    "        参数:\n",
    "            text: 需要编码的字符串。\n",
    "\n",
    "        返回:\n",
    "            token id的列表（List[int]）。\n",
    "\n",
    "        示例:\n",
    "            假设tokenizer已经初始化好:\n",
    "            tokenizer.encode(\"ab\")\n",
    "            可能返回: [2]\n",
    "        \"\"\"\n",
    "        pass  # 这里只是示例，实际实现略\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        将token id的列表解码为原始字符串。\n",
    "\n",
    "        参数:\n",
    "            ids: token id的列表。\n",
    "\n",
    "        返回:\n",
    "            解码后的字符串。\n",
    "\n",
    "        示例:\n",
    "            假设tokenizer已经初始化好:\n",
    "            tokenizer.decode([2])\n",
    "            可能返回: \"ab\"\n",
    "        \"\"\"\n",
    "        pass  # 这里只是示例，实际实现略\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
