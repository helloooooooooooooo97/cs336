{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1讲解（中文）\n",
    "\n",
    "1. 任务简介：说明具体的任务\n",
    "2. 核心函数：给出核心函数并举例说明输入输出\n",
    "3. 常见难点：讨论问题难点\n",
    "4. 作业内容：课程中涉及到的作业\n",
    "\n",
    "## 1. 任务简介\n",
    "\n",
    "p1主要实现了一个基础的BPE（Byte Pair Encoding）分词器的训练与推理流程，核心包括以下几个部分：\n",
    "\n",
    "1. BPE训练（train_bpe）：\n",
    "   - 输入：原始文本数据、目标词表大小、特殊token列表。\n",
    "   - 步骤：\n",
    "     a. 统计所有单词的频率，并将每个单词拆分为字节（bytes）序列。\n",
    "     b. 初始化词表为所有出现过的字节（以及特殊token）。\n",
    "     c. 不断统计所有相邻字节对（pair）的出现频率，选择频率最高的pair进行合并（merge），并记录merge顺序。\n",
    "     d. 每次merge后更新所有token序列，直到词表大小达到目标或无法继续合并。\n",
    "     e. 最终输出词表（vocab，id到bytes的映射）和merges（合并顺序列表）。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Tokenizer实现（Tokenizer类）：\n",
    "   - 初始化时接收vocab、merges和特殊token，构建token到id的映射和BPE合并规则。\n",
    "   - encode方法：将输入字符串编码为token id序列，优先匹配最长的特殊token，其余部分用BPE规则分词。\n",
    "   - decode方法：将token id序列还原为原始字符串。\n",
    "\n",
    "3. 适配器与测试：\n",
    "   - 提供了run_train_bpe等适配器函数，方便测试用例调用训练流程。\n",
    "   - 测试用例会对分词器的训练速度、分词准确性、特殊token处理、与tiktoken一致性等进行全面验证。\n",
    "\n",
    "4. 关键点说明：\n",
    "   - 特殊token始终保持原子性，不会被BPE规则拆分或合并。\n",
    "   - vocab和merges的格式与GPT-2等主流BPE分词器兼容，便于对比和验证。\n",
    "   - 支持unicode字符串，能够正确处理多语言和emoji等复杂字符。\n",
    "\n",
    "整体流程：先用train_bpe训练得到vocab和merges，再用Tokenizer进行分词和还原，确保与主流实现一致，并通过丰富的测试用例验证正确性和健壮性。\n",
    "\n",
    "## 2.核心代码\n",
    "\n",
    "### 2.1 train_bpe \n",
    "\n",
    "1. 输入输出\n",
    "\n",
    "```python\n",
    "  # 输入示例\n",
    "    data = \"hello world world\"\n",
    "    vocab_size = 10\n",
    "    special_tokens = [\"<|endoftext|>\"]\n",
    "    vocab, merges = train_bpe(data, vocab_size, special_tokens)\n",
    "\n",
    "  # 输出示例\n",
    "    vocab = {\n",
    "        0: b\"<|endoftext|>\",\n",
    "        1: b\"h\",\n",
    "        2: b\"e\",\n",
    "        3: b\"l\",\n",
    "        4: b\"o\",\n",
    "        5: b\"w\",\n",
    "        6: b\"r\",\n",
    "        7: b\"d\",\n",
    "        8: b\"wo\",\n",
    "        9: b\"rl\"\n",
    "    }\n",
    "    merges = [\n",
    "        (b\"w\", b\"o\"),\n",
    "        (b\"r\", b\"l\"),\n",
    "        ...\n",
    "    ]\n",
    "  ```\n",
    "2. 实现代码：`train_bpe(data: str, vocab_size: int, special_tokens: Optional[List[str]] = None) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]`\n",
    "  - 作用：训练BPE分词器，返回词表（vocab）和合并规则（merges）。\n",
    "\n",
    "\n",
    "3. 测试方式：通过`test_train_bpe.py`的所有测试文件即可通过全部测试\n",
    "\n",
    "### 2.2 Tokenizer\n",
    "\n",
    "1. 输入输出\n",
    "``` python\n",
    "# encode方法输入输出示例：\n",
    "  tokenizer = Tokenizer(vocab, merges, special_tokens)\n",
    "  text = \"hello world\"\n",
    "  token_ids = tokenizer.encode(text)\n",
    "  # 输出: [1, 2, 3, 3, 4, 5, 4, 6, 3, 7]\n",
    "\n",
    "# decode方法输入输出示例：\n",
    "  recovered = tokenizer.decode(token_ids)\n",
    "  # 输出: \"hello world\"\n",
    "```\n",
    "\n",
    "2. 实现代码：`Tokenizer(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], special_tokens: Optional[List[str]] = None)`\n",
    "   - 作用：BPE分词器类，实现分词和还原。\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "3. 测试方式：通过`test_tokenizer.py`的所有测试文件即可通过全部测试\n",
    "\n",
    "### 代码难点\n",
    "\n",
    "\n",
    "\n",
    "### 作业问答"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
